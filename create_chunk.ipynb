{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvIMPSDEj6WO",
        "outputId": "efbcf11f-0616-4bd0-fa4e-d3473c5c4c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trSlIMT_U8B1",
        "outputId": "c385f7d7-f8d1-4ec0-b258-b5728650843e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxobNld2eYlU",
        "outputId": "bfc40261-266d-4238-9c1d-a0f71c7de6ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 18041 entries, 0 to 18040\n",
            "Data columns (total 6 columns):\n",
            " #   Column              Non-Null Count  Dtype \n",
            "---  ------              --------------  ----- \n",
            " 0   job_description     18041 non-null  object\n",
            " 1   lower_description   18041 non-null  object\n",
            " 2   word_tokenized      18041 non-null  object\n",
            " 3   sentence_tokenized  18041 non-null  object\n",
            " 4   clean_stopword      18041 non-null  object\n",
            " 5   clean_lemmed        18041 non-null  object\n",
            "dtypes: object(6)\n",
            "memory usage: 845.8+ KB\n"
          ]
        }
      ],
      "source": [
        "df2 = pd.read_csv('/content/drive/MyDrive/WEB SCRAPING 101/job_description_only_after_cleaning.csv')\n",
        "df2.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrZ8od8-AVFe"
      },
      "outputs": [],
      "source": [
        "df2.drop_duplicates('lower_description', inplace= True)\n",
        "df2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMz98Ii7FhO2"
      },
      "source": [
        "# POS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84nC3dhEePhF"
      },
      "outputs": [],
      "source": [
        "def pos_series(keyword):\n",
        "    '''categorizes after tokenizing words with POS tags'''\n",
        "    tokens = nltk.word_tokenize(keyword)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    return tagged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha3ngJ7NeP4v"
      },
      "outputs": [],
      "source": [
        "pos_tagged_arrs = df2.lower_description.apply(pos_series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMjUNxnReQT4"
      },
      "outputs": [],
      "source": [
        "pos_tagged = []\n",
        "for row in pos_tagged_arrs.values:\n",
        "    for element in row:\n",
        "        pos_tagged.append(element)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM4QXbWNeviQ"
      },
      "outputs": [],
      "source": [
        "pos_df = pd.DataFrame(pos_tagged, columns = ('word','POS'))\n",
        "\n",
        "# remove unnecessary char\n",
        "char_removal = [',', '.', ':', '#', '$', '\\'\\'', '``', '(', ')']\n",
        "drop_indices = (pos_df.loc[pos_df.POS.isin(char_removal)].index)\n",
        "pos_df.drop(drop_indices, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeEReq61HT1r"
      },
      "source": [
        "# Noun Prase #1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1n4F_64VS7q"
      },
      "outputs": [],
      "source": [
        "# defining grammer within the text using reg expressions\n",
        "# optional determinate, any number of adjectives, a noun, noun plural, proper noun with additionals following\n",
        "grammar1 = ('''Noun Phrases: {<DT>?<JJ>*<NN|NNS|NNP>+}''')\n",
        "chunkParser = nltk.RegexpParser(grammar1)\n",
        "tree1 = chunkParser.parse(pos_tagged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhQm0Y6EYG8T"
      },
      "outputs": [],
      "source": [
        "# typical noun phrase pattern to be pickled for later analyses\n",
        "g1_chunks = []\n",
        "for subtree in tree1.subtrees(filter=lambda t: t.label() == 'Noun Phrases'):\n",
        "    # print(subtree)\n",
        "    g1_chunks.append(subtree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mqb7BCe2mr95"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/My Drive/chunks_1.pickle', 'wb') as fp1:\n",
        "    pickle.dump(g1_chunks, fp1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8AsEdg8Hdpo"
      },
      "source": [
        "# Noun Phrase #2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7AuWWmCih7w"
      },
      "outputs": [],
      "source": [
        "# Noun phrase variation\n",
        "# preposition maybe, any number of adjective or nouns, any plural nouns or singular nouns\n",
        "grammar2 = ('''NP2: {<IN>?<JJ|NN>*<NNS|NN>} ''')\n",
        "chunkParser = nltk.RegexpParser(grammar2)\n",
        "tree2 = chunkParser.parse(pos_tagged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZtufY7eYKwJ"
      },
      "outputs": [],
      "source": [
        "# variation of a noun phrase pattern to be pickled for later analyses\n",
        "g2_chunks = []\n",
        "for subtree in tree2.subtrees(filter=lambda t: t.label() == 'NP2'):\n",
        "    # print(subtree)\n",
        "    g2_chunks.append(subtree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQ8ApP1fmyod"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/My Drive/chunks_2.pickle', 'wb') as fp2:\n",
        "    pickle.dump(g2_chunks , fp2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUPMERSrHhiF"
      },
      "source": [
        "Verb-Noun Pattern #3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLkcIxO0VTO9"
      },
      "outputs": [],
      "source": [
        "# any sort of verb followed by any number of nouns\n",
        "grammar3 = ('''\n",
        "    VS: {<VBG|VBZ|VBP|VBD|VB|VBN><NNS|NN>*}\n",
        "    ''')\n",
        "chunkParser = nltk.RegexpParser(grammar3)\n",
        "tree3 = chunkParser.parse(pos_tagged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pR3EFTcAjN1C"
      },
      "outputs": [],
      "source": [
        "# verb-noun pattern to be pickled for later analyses\n",
        "g3_chunks = []\n",
        "for subtree in tree3.subtrees(filter=lambda t: t.label() == 'VS'):\n",
        "    # print(subtree)\n",
        "    g3_chunks.append(subtree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UooPLPB0m6p_"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/My Drive/chunks_3.pickle', 'wb') as fp3:\n",
        "    pickle.dump(g3_chunks, fp3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebFXJ-PQHmqh"
      },
      "source": [
        "# <noun, noun*> Pattern #4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmrbtjenVTfR"
      },
      "outputs": [],
      "source": [
        "# any number of a singular or plural noun followed by a comma followed by the same noun, noun, noun pattern\n",
        "grammar4 = ('''\n",
        "    Commas: {<NN|NNS>*<,><NN|NNS>*<,><NN|NNS>*}\n",
        "    ''')\n",
        "chunkParser = nltk.RegexpParser(grammar4)\n",
        "tree4 = chunkParser.parse(pos_tagged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WOwZEb8kHLl"
      },
      "outputs": [],
      "source": [
        "# common pattern of listing skills to be pickled for later analyses\n",
        "g4_chunks = []\n",
        "for subtree in tree4.subtrees(filter=lambda t: t.label() == 'Commas'):\n",
        "    # print(subtree)\n",
        "    g4_chunks.append(subtree)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Krjr8UmgnYnS"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/My Drive/chunks_4.pickle', 'wb') as fp4:\n",
        "    pickle.dump(g4_chunks, fp4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Chunk"
      ],
      "metadata": {
        "id": "132OpYyELuxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks1 = pickle.load( open('/content/drive/MyDrive/chunks_1.pickle', \"rb\" ) )\n",
        "chunks2 = pickle.load( open('/content/drive/MyDrive/chunks_2.pickle', \"rb\" ) )\n",
        "chunks3 = pickle.load( open('/content/drive/MyDrive/chunks_3.pickle', \"rb\" ) )\n",
        "chunks4 = pickle.load( open('/content/drive/MyDrive/chunks_4.pickle', \"rb\" ) )"
      ],
      "metadata": {
        "id": "CXTXRmmgTzWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks1"
      ],
      "metadata": {
        "id": "jvptpZAcMADz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Length:', len(chunks1), 'Sample Size:', len(chunks1) * .10)\n",
        "print('Length:', len(chunks2), 'Sample Size:', len(chunks2) * .10)\n",
        "print('Length:', len(chunks3), 'Sample Size:', len(chunks3) * .10)\n",
        "print('Length:', len(chunks4), 'Sample Size:', len(chunks4) * .10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR2ZNz_PMES0",
        "outputId": "bbaed309-cd2e-4368-87d9-ff07c293bd0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length: 1418223 Sample Size: 141822.30000000002\n",
            "Length: 1453992 Sample Size: 145399.2\n",
            "Length: 707018 Sample Size: 70701.8\n",
            "Length: 60326 Sample Size: 6032.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KyN9iQCoM8zM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}